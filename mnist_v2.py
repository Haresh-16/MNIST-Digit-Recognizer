# -*- coding: utf-8 -*-
"""
Created on Thu May 14 01:40:22 2020

@author: Indrajithu
"""

# -*- coding: utf-8 -*-
"""mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1baqBw9FU9NeJu5YyjhEu79ubvYIzisog

# **IMPORTING NECESSARY LIBRARIES**
"""

import keras
import seaborn as sns
import numpy as np
import pandas as pd
from keras.utils import to_categorical
from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten
from keras.optimizers import Adam
from keras.models import Sequential
from matplotlib import pyplot as plt

train=pd.read_csv("E:\MNIST GitHub/train.csv")
#The data is read from the second row onwards , Since the first row contains headers [label,pixel0,pixel1,...]
trainX=train.iloc[1:,1:]
trainY=train.iloc[1:,0]

"""# Getting Data ready!"""

#Determining output labels
labels=np.unique(list(trainY))
num_class=len(labels)

#Convert the pd dataframes into numpy arrays
trainX=np.array(trainX)
trainY=np.array(trainY)

#Deciding size of the Validation and Test sets
val_size=int(len(trainX)*0.1)
test_size=int(len(trainX)*0.2)

#Validation data preparation
valX=trainX[:val_size].copy()
valY=trainY[:val_size].copy()
trainX=trainX[val_size:]
trainY=trainY[val_size:]

#Test Data Preparation
testX=trainX[:test_size].copy()
testY=trainY[:test_size].copy()
trainX=trainX[test_size:]
trainY=trainY[test_size:]

#Reshaping the row pixel values into a matrix where each matrix constitutes an image of a handwritten digit 
trainX=trainX.reshape((-1,28,28,1))
valX=valX.reshape((-1,28,28,1))
testX=testX.reshape((-1,28,28,1))

#Since the output variable is to categorical , I'm converting them into One Hot Encoded Form. This will facilitate is outputting 
#the predictions in human-understanble form(Will become clear during the prediction.)
trainY=to_categorical(trainY,num_classes=10)
testY=to_categorical(testY,num_classes=10)
valY=to_categorical(valY,num_classes=10)

"""# MODEL ARCHITECTURE"""
from keras.layers import Dropout
#Initialising an instance of class Sequential to hold the model layers
model=Sequential()

##Starting to add the model layers
from keras.layers import BatchNormalization
#Adding the first Conv layer with a large kernel size of (7,7) to capture the large features 
model.add(Conv2D(8,(7,7),input_shape=(28,28,1),activation='relu',padding='same'))
model.add(BatchNormalization())
#Adding MaxPooling to reduce the dimensionality of feature map to contain the dominant features
model.add(MaxPooling2D((2,2)))

#Two more consecutive Conv layers are added to deepen the networks capturing better
#representational features of the data
model.add(Conv2D(32,(5,5),activation='relu',padding='same'))
model.add(BatchNormalization())

model.add(MaxPooling2D((2,2)))

model.add(Conv2D(32,(5,5),activation='relu',padding='same'))
model.add(BatchNormalization())

model.add(MaxPooling2D((2,2)))

#Fully connected layers to process feature maps from Conv layers
model.add(Flatten())
model.add(Dense(128,activation='relu'))
model.add(BatchNormalization())

model.add(Dense(64,activation='relu'))
model.add(BatchNormalization())

model.add(Dense(32,activation='relu'))
model.add(BatchNormalization())

model.add(Dense(16,activation='relu'))
model.add(BatchNormalization())

#Final fully connected layer to arrive at class probabilities across 10 classes (Digits 0 - 9)
model.add(Dense(10,activation='softmax'))

"""# ADDING ADAM OPTIMISATION FOR DESCENT IN THE LOSS LANDSCAPE AND STARTING THE DESCENT TOWARDS LEAST LOSS BY LEARNING PARAMETERS ON TRAINING ON THE TRAIN DATA

![](https://www.researchgate.net/profile/Alexander_Amini/publication/325142728/figure/fig1/AS:766109435326465@1559666131320/Non-convex-optimization-We-utilize-stochastic-gradient-descent-to-find-a-local-optimum.jpg)

Image courtesy of Spatial Uncertainty Sampling for End-to-End Control ,Alexander Amini
"""

from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import Callback
from keras import backend
from math import pi
from math import cos
from math import floor

# snapshot ensemble with custom learning rate schedule
class SnapshotEnsemble(Callback):
	# constructor
	def __init__(self, n_epochs, n_cycles, lrate_max, verbose=0):
		self.epochs = n_epochs
		self.cycles = n_cycles
		self.lr_max = lrate_max
		self.lrates = list()

	# calculate learning rate for epoch
	def cosine_annealing(self, epoch, n_epochs, n_cycles, lrate_max):
		epochs_per_cycle = floor(n_epochs/n_cycles)
		cos_inner = (pi * (epoch % epochs_per_cycle)) / (epochs_per_cycle)
		return lrate_max/2 * (cos(cos_inner) + 1)

	# calculate and set learning rate at the start of the epoch
	def on_epoch_begin(self, epoch, logs={}):
		# calculate learning rate
		lr = self.cosine_annealing(epoch, self.epochs, self.cycles, self.lr_max)
		# set learning rate
		backend.set_value(self.model.optimizer.lr, lr)
		# log value
		self.lrates.append(lr)

	# save models at the end of each cycle
	def on_epoch_end(self, epoch, logs={}):
		# check if we can save model
		epochs_per_cycle = floor(self.epochs / self.cycles)
		if epoch != 0 and (epoch + 1) % epochs_per_cycle == 0:
			# save model to file
			filename = "snapshot_model_%d.h5" % int((epoch + 1) / epochs_per_cycle)
			self.model.save(filename)
			print('>saved snapshot %s, epoch %d' % (filename, epoch))

ca = SnapshotEnsemble(20, 5, 1e-3)

#Adam optimiser with learning rate = 0.001
opt=Adam(lr=1e-3)
#Compiling model. Loss is Categorical crossentropy since we are predicting across more than two classes
model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])
#Fitting model on training data and the learned weights on train data is used to make predictions on validation data
#for making sure that model is generalising(Will become clear while plotting Accuracy v/s Epochs graph)

H=model.fit(x=trainX,y=trainY,validation_data=(valX,valY),epochs=20,batch_size=32,callbacks=[ca])

from keras.models import load_model
#ENSEMBLE PREDICTION
def load_all_models(n_models):
	all_models = list()
	for i in range(n_models):
		# define filename for this ensemble
		filename = 'snapshot_model_' + str(i + 1) + '.h5'
		# load model from file
		model = load_model(filename)
		# add to list of members
		all_models.append(model)
		print('>loaded %s' % filename)
	return all_models

# make an ensemble prediction for multi-class classification
def ensemble_predictions(members, testX):
	# make predictions
	yhats = [model.predict(testX) for model in members]
	yhats = np.array(yhats)
	# sum across ensemble members
	summed = np.sum(yhats, axis=0)
	# argmax across classes
	result = np.argmax(summed, axis=1)
	return result

from sklearn.metrics import accuracy_score
# evaluate a specific number of members in an ensemble
def evaluate_n_members(members, n_members, testX, testy):
	# select a subset of members
	subset = members[:n_members]
	# make prediction
	yhat = ensemble_predictions(subset, testX)
	# calculate accuracy
	return accuracy_score(np.argmax(testy,axis=1), yhat)

# load models in order
members = load_all_models(4)
print('Loaded %d models' % len(members))
# reverse loaded models so we build the ensemble with the last models first
members = list(reversed(members))
# evaluate different numbers of ensembles on hold out set
single_scores, ensemble_scores = list(), list()
for i in range(1, len(members)+1):
	# evaluate model with i members
	ensemble_score = evaluate_n_members(members, i, testX, testY)
	# evaluate the i'th model standalone
	_, single_score = members[i-1].evaluate(testX, testY, verbose=0)
	# summarize this step
	print('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))
	ensemble_scores.append(ensemble_score)
	single_scores.append(single_score)
# summarize average accuracy of a single final model
print('Accuracy %.3f (%.3f)' % (np.mean(single_scores), np.std(single_scores)))
# plot score vs number of ensemble members
x_axis = [i for i in range(1, len(members)+1)]
from matplotlib import pyplot
pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')
pyplot.plot(x_axis, ensemble_scores, marker='o')
pyplot.show()
'''
plt.style.use("ggplot")
plt.figure()
#Since I'm are training for only 20 epochs the the np.arange plots values from 0 to 20 in x-axis
#With the Accuracy in y-axis
#Training accuracy
plt.plot(np.arange(0,20),H.history['accuracy'],label="train_acc")
#Validation accuracy
plt.plot(np.arange(0,20),H.history['val_accuracy'],label="val_acc")
plt.title("Accuracy v/s Epochs")
plt.xlabel("Epoch#")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

plt.style.use("ggplot")
plt.figure()
#Same as Accuracy plotting 
#However here loss in the y-axis
#Training loss
plt.plot(np.arange(0,20),H.history['loss'],label="train_loss")
#Validation loss
plt.plot(np.arange(0,20),H.history['val_loss'],label="val_loss")
plt.title("Loss v/s Epochs")
plt.xlabel("Epoch#")
plt.ylabel("Loss")
plt.legend()
plt.show()
'''
"""We can see in the above loss graph that the val loss and train loss are comparitively close showing that there is less variance. The accuracy graph shows that the model has less bias since the train accuracy and val accuracy are close.All this means that the model is a pretty good fit. SEE https://www.kaggle.com/residentmario/bias-variance-tradeoff for more details on this.

Now , it's time to make the submission for the Digit Recognizer competition to take a position in the leader board

# LOADING THE TEST DATA
"""

import pandas as pd
test=pd.read_csv("E:\MNIST GitHub/test.csv")

testX=test.iloc[0:,:]

"""Reshaping done similar to the train data. Keep in mind that whatever preprocessing the train data was subjected to , the same should be done on the test data.Refer https://cs231n.github.io/neural-networks-2/#datapre for more details on this

**Prediction** done on test data using model
"""

testX=np.array(testX)
testX=testX.reshape((-1,28,28,1))
pred=ensemble_predictions(members,testX)

"""Using np.argmax to find at which values in the rows of 'pred' variable have highest probability value , meaning , the model has highest confidence for the data sample belonging to that particular class obtained by using np.argmax()"""


"""We can see above that the pred variable holds predictions for 28000 test data samples.

# MAKING THE NUMPY ARRAY INTO A CSV FILE
"""

test_result=pd.Series(pred,name="Label")


table=pd.concat([pd.Series(range(1,28001),name="ImageId"),test_result],axis=1)


table=table.to_csv("E:\MNIST GitHub/Test results.csv",index=False)

